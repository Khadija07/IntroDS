{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vo4EsyVFw0iV"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary sentiment classification: \"Do the following texts express a positive or negative sentiment?\"\n"
      ],
      "metadata": {
        "id": "kVxmljnIw2Su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load a pretrained model and tokenizer suitable for sentiment analysis\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model_sentiment = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer_sentiment = AutoTokenizer.from_pretrained(model_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "tb5A9yAkw7a1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model_sentiment,\n",
        "    tokenizer=tokenizer_sentiment\n",
        ")"
      ],
      "metadata": {
        "id": "pMf5-gP-9ny-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "TBwtG1HvXylB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one shot\n",
        "text = \"you are very good!\"\n",
        "\n",
        "tokenized_texts = tokenizer_sentiment(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Passing the tokenized texts through the model to get sentiment scores\n",
        "with torch.no_grad():\n",
        "    logits = model_sentiment(**tokenized_texts).logits\n",
        "\n",
        "sentiment_probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "positive_prob = sentiment_probabilities[0][1].item()\n",
        "negative_prob = sentiment_probabilities[0][0].item()\n",
        "\n",
        "sentiment = \"Positive\" if positive_prob > negative_prob else \"Negative\"\n",
        "print(\"Do the following texts express a positive or negative sentiment?\")\n",
        "print(f\"Text: {text}\\nResult: {sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6VMhB3pw_3M",
        "outputId": "fb9d78b9-c367-449d-a9ed-f219db88be86"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do the following texts express a positive or negative sentiment?\n",
            "Text: you are very good!\n",
            "Result: Positive\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# two shots\n",
        "texts = [\"very good product!\", \"worst thing ever\"]\n",
        "\n",
        "tokenized_texts = tokenizer_sentiment(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Passing the tokenized texts through the model to get sentiment scores\n",
        "with torch.no_grad():\n",
        "    logits = model_sentiment(**tokenized_texts).logits\n",
        "\n",
        "sentiment_probabilities = torch.softmax(logits, dim=1)\n",
        "print(\"Do the following texts express a positive or negative sentiment?\")\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    positive_prob = sentiment_probabilities[i][1].item()\n",
        "    negative_prob = sentiment_probabilities[i][0].item()\n",
        "\n",
        "    sentiment = \"Positive\" if positive_prob > negative_prob else \"Negative\"\n",
        "\n",
        "    print(f\"Text: {text}\\nResult: {sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiTWO5zyxD_P",
        "outputId": "9b576037-0f19-4c9d-c146-a9b49fa5d03e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do the following texts express a positive or negative sentiment?\n",
            "Text: very good product!\n",
            "Result: Negative\n",
            "\n",
            "Text: worst thing ever\n",
            "Result: Negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "person name recognition: \"List the person names occurring in the following texts.\"\n"
      ],
      "metadata": {
        "id": "mZSXHgfExG0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "MODEL_NAME = 'gpt2-large'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=model.device\n",
        ")"
      ],
      "metadata": {
        "id": "FurE-ZYVxJbs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#zero shot\n",
        "\n",
        "text = \"My name is Khadija\"\n",
        "prompt = f\"List the person names occurring in the following texts.\\nText = {text}Answer:\"\n",
        "\n",
        "output = pipe(prompt, max_length=40)\n",
        "\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs7GdNXExMoo",
        "outputId": "16bcd7d3-ad55-458b-ea1a-99d014556ddc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List the person names occurring in the following texts.\n",
            "Text = My name is KhadijaAnswer: 1: Khadija is my name\n",
            "Text = My father is Shams Answer: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one shot\n",
        "text1= \"My name is Khadija\"\n",
        "result1 = \"Khadija\"\n",
        "text2 = \"your name is Pinky\"\n",
        "prompt = f\"List the person names occurring in the following texts.\\nText = {text1} Answer: {result1}\\nText = {text2} Answer: \"\n",
        "\n",
        "output = pipe(prompt, max_length=38)\n",
        "\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya1Pom93xQCm",
        "outputId": "98c84d8b-c353-4c46-eb77-413fbd7cf1a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List the person names occurring in the following texts.\n",
            "Text = My name is Khadija Answer: Khadija\n",
            "Text = your name is Pinky Answer:  Pinky\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#two shot\n",
        "\n",
        "text1= \"The girl name Lubna is good\"\n",
        "result1 = \"Lubna\"\n",
        "text2 = \"His classmate has a name of Rahul\"\n",
        "result2 = \"Rahul\"\n",
        "text3= \"The name of the boy who called me is Peter\"\n",
        "prompt = f\"List the person names occurring in the following texts.\\nText = {text1} Answer: {result1}\\nText = {text2} Answer: {result2}\\nText = {text3} Answer:\"\n",
        "\n",
        "output = pipe(prompt, max_length=50)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKesoEydxTkq",
        "outputId": "7c747ceb-9059-4261-e080-dfa5a1314aa0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 52, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List the person names occurring in the following texts.\n",
            "Text = The girl name Lubna is good Answer: Lubna\n",
            "Text = His classmate has a name of Rahul Answer: Rahul\n",
            "Text = The name of the boy who called me is Peter Answer: Peter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-digit addition: \"This is a first grade math exam.\"\n"
      ],
      "metadata": {
        "id": "wWPvH2W_eOJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#one shot\n",
        "\n",
        "question1 = \"Find the sum of 5 and 2\"\n",
        "answer1 = \"7\"\n",
        "\n",
        "question2 = \"Find the sum of 5 and 8\"\n",
        "\n",
        "prompt = f\"Question: {question1} Answer: {answer1}\\nQuestion: {question2} Answer: \"\n",
        "\n",
        "output = pipe(prompt, max_length=30)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVQ0ppyrcuac",
        "outputId": "188c6b3d-6584-4b60-8af0-a83a76e64fcd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Find the sum of 5 and 2 Answer: 7\n",
            "Question: Find the sum of 5 and 8 Answer: ???\n",
            "Question: Find\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#two shot\n",
        "\n",
        "question1 = \"Find the sum of 5 and 2\"\n",
        "answer1 = \"7\"\n",
        "\n",
        "question2 = \"Find the sum of 5 and 8\"\n",
        "answer2 = \"13\"\n",
        "\n",
        "question3 = \"Find the sum of 1 and 2\"\n",
        "\n",
        "prompt = f\"Question: {question1} Answer: {answer1}\\nQuestion: {question2} Answer: {answer2}\\nQuestion: {question3} Answer: \"\n",
        "\n",
        "output = pipe(prompt, max_length=50)\n",
        "\n",
        "print(output[0]['generated_text'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbhFK4cEysmX",
        "outputId": "0794ab55-445a-4113-ec8e-765d2c8c1c86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Find the sum of 5 and 2 Answer: 7\n",
            "Question: Find the sum of 5 and 8 Answer: 13\n",
            "Question: Find the sum of 1 and 2 Answer:  15\n",
            "Question: Find the sum of 2 and 7\n"
          ]
        }
      ]
    }
  ]
}