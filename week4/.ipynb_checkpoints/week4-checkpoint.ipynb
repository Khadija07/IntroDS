{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vo4EsyVFw0iV"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVxmljnIw2Su"
   },
   "source": [
    "binary sentiment classification: \"Do the following texts express a positive or negative sentiment?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb5A9yAkw7a1"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a pretrained model and tokenizer suitable for sentiment analysis\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6VMhB3pw_3M"
   },
   "outputs": [],
   "source": [
    "#one shot\n",
    "text = \"This product is terrible!\"\n",
    "\n",
    "tokenized_texts = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Passing the tokenized texts through the model to get sentiment scores\n",
    "with torch.no_grad():\n",
    "    logits = model(**tokenized_texts).logits\n",
    "\n",
    "sentiment_probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "positive_prob = sentiment_probabilities[0][1].item()\n",
    "negative_prob = sentiment_probabilities[0][0].item()\n",
    "\n",
    "sentiment = \"Positive\" if positive_prob > negative_prob else \"Negative\"\n",
    "print(\"Do the following texts express a positive or negative sentiment?\")\n",
    "print(f\"Text: {text}\\nResult: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiTWO5zyxD_P"
   },
   "outputs": [],
   "source": [
    "# two shots\n",
    "texts = [\"I love this product!\", \"This movie is terrible.\"]\n",
    "\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Passing the tokenized texts through the model to get sentiment scores\n",
    "with torch.no_grad():\n",
    "    logits = model(**tokenized_texts).logits\n",
    "\n",
    "sentiment_probabilities = torch.softmax(logits, dim=1)\n",
    "print(\"Do the following texts express a positive or negative sentiment?\")\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    positive_prob = sentiment_probabilities[i][1].item()\n",
    "    negative_prob = sentiment_probabilities[i][0].item()\n",
    "\n",
    "    sentiment = \"Positive\" if positive_prob > negative_prob else \"Negative\"\n",
    "\n",
    "    print(f\"Text: {text}\\nResult: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZSXHgfExG0Q"
   },
   "source": [
    "person name recognition: \"List the person names occurring in the following texts.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FurE-ZYVxJbs"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "MODEL_NAME = 'gpt2-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs7GdNXExMoo"
   },
   "outputs": [],
   "source": [
    "#zero shot\n",
    "\n",
    "text = \"My name is Khadija\"\n",
    "prompt = f\"List the person names occurring in the following texts.\\nText = {text}Answer:\"\n",
    "\n",
    "output = pipe(prompt, max_length=40)\n",
    "\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya1Pom93xQCm"
   },
   "outputs": [],
   "source": [
    "#one shot\n",
    "text1= \"My name is Khadija\"\n",
    "result1 = \"Khadija\"\n",
    "text2 = \"your name is Ayesha\"\n",
    "prompt = f\"List the person names occurring in the following texts.\\nText = {text1} Answer: {result1}\\nText = {text2} Answer: \"\n",
    "\n",
    "output = pipe(prompt, max_length=40)\n",
    "\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKesoEydxTkq"
   },
   "outputs": [],
   "source": [
    "#two shot\n",
    "\n",
    "text1= \"The girl name Lubna is good\"\n",
    "result1 = \"Lubna\"\n",
    "text2 = \"His classmate has a name of Rahul\"\n",
    "result2 = \"Rahul\"\n",
    "text3= \"The name of the boy who called me is Peter\"\n",
    "prompt = f\"List the person names occurring in the following texts.\\nText = {text1} Answer: {result1}\\nText = {text2} Answer: {result2}\\nText = {text3} Answer:\"\n",
    "\n",
    "output = pipe(prompt, max_length=50)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
